# -*- coding: utf-8 -*-
"""EDA

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1yVXfU4iLLTPija_LfpX9cSTweN6WJcXO
"""

from google.colab import drive
drive.mount('/content/drive')
import os
os.chdir("drive/MyDrive/Bigdata/Project")

import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt

"""# Import, Clean and Join"""

demographic_data = pd.read_excel("whole_data/whole_data/original/Demography_USA.xlsx")
pharma_data = pd.read_excel("whole_data/whole_data/original/Modified_pharmacies_data.xlsx")

unwanted_words = ['borough', 'city and borough', 'municipality','census area','county', 'city']
pharma_data['county'] = pharma_data['county'].str.lower()
pharma_data['State'] = pharma_data['State'].str.lower()
demographic_data['County Name'] = demographic_data['County Name'].str.lower()
demographic_data['STATE_NAME'] = demographic_data['STATE_NAME'].str.lower()
pharma_data["county"] = pharma_data["county"].replace(dict(zip(unwanted_words,['']*len(unwanted_words))),regex=True)

pharma_data['county'] = pharma_data['county'].str.strip()
pharma_data['State'] = pharma_data['State'].str.strip()
print(pharma_data.shape)
print(pharma_data.columns)
print(demographic_data.shape)
print(demographic_data.columns)

"""# Analysis on Merge data"""

merge_data_del = pd.merge(left=pharma_data, right=demographic_data, left_on=['county', 'State'], right_on=['County Name', 'STATE_NAME'], how = 'left')
 print(merge_data_del.shape)
 print('')
 print('Missing values after join by each state')
 missing_joins = merge_data_del[merge_data_del['County Name'].isnull()][['county','State_x', 'County Name', 'STATE_NAME']].drop_duplicates().groupby('State_x')['county'].count()
 del1 = merge_data_del.groupby('State_x')['county'].count()
 del1 = pd.concat([missing_joins, del1], axis = 1)
 del1.columns = ['miss count', 'overall count']
 del1['%miss'] = del1['miss count']/del1['overall count']*100
 del1

merge_data_del[merge_data_del['County Name'].isnull()]

"""### Observation
#### From the above table, we can observe more than 4% of values are missing from alaska, north carolina, oregon. For now, we will be excluding the values which has miss % more than 1
"""

merge_data_del2 = pd.merge(left=pharma_data, right=demographic_data, left_on=['county', 'State'], right_on=['County Name', 'STATE_NAME'], how = 'inner')
merge_data_del2[['county','State_x', 'Pharmacy Name', 'Street Address 1 (physical store address)', 'Zip']].groupby(['county','State_x', 'Pharmacy Name', 'Street Address 1 (physical store address)'])['county'].count().sort_values()

"""### Observation
#### Only last 3 values of the whole data have multiple entries for the same pharmacy and address. In total, there are 4 additional values which are negligible compared to whole.

# Data Merge and Observations based on correlation values
"""

# merge_data = pd.merge(left=pharma_summary_data, right=demographic_data, on='FIPS', how = 'inner')
# merge_data.rename({'Pharmacy_count':'count_pharma'},axis=1, inplace=True)

pharma_data_group = pharma_data.groupby(['State', 'county'], as_index = False)['Pharmacy Name'].count().rename({'Pharmacy Name' : 'count_pharma'}, axis = 1)
merge_data1 = pd.merge(left=pharma_data_group, right=demographic_data, left_on=['county', 'State'], right_on=['County Name', 'STATE_NAME'], how = 'inner')
merge_data = merge_data1[~merge_data1['State_x'].isin(['north carolina', 'oregon', 'alaska'])]

corr_with_pharma = merge_data.corr().reset_index()[['index', 'count_pharma']]
states_list = list(set(merge_data.STATE_NAME))
for s in states_list:
  df_corr = merge_data[merge_data.STATE_NAME == s].corr(numeric_only=True).reset_index()[['index', 'count_pharma']].rename({'count_pharma' : s}, axis = 1)
  corr_with_pharma = pd.concat([corr_with_pharma, df_corr.drop('index', axis=1)],axis=1)
corr_with_pharma.set_index('index', inplace=True)
corr_with_pharma.index

merge_data.to_csv('merge.csv')

fig, ax = plt.subplots(figsize=(16, 4))
sns.heatmap(corr_with_pharma.loc[['WHITE', 'BLACK', 'AMERI_ES', 'ASIAN', 'HAWN_PI', 'HISPANIC', 'OTHER', 'MULT_RACE']], annot=True, linewidths=0.3)

"""### Observation(based on race)

1.   No descrimination observed in Rhode islands
2.   People of race Ameri_es are not considered in maine and montana

1.   Only Hawn_pi race is downsided in texas and california
2.   Most descrimination is observed in arkansas and missouri





"""

fig, ax = plt.subplots(figsize=(16, 4))
sns.heatmap(corr_with_pharma.loc[['AGE_UNDER5', 'AGE_5_9', 'AGE_10_14', 'AGE_15_19', 'AGE_20_24', 'AGE_25_34', 'AGE_35_44', 'AGE_45_54', 'AGE_55_64', 'AGE_65_74', 'AGE_75_84', 'AGE_85_UP']], annot=True, linewidths=0.3)

"""## Observation(Based on Age)



*   No major descrimination based on age is observed




"""

fig, ax = plt.subplots(figsize=(16, 3))
sns.heatmap(corr_with_pharma.loc[['Prevalence of obesity', 'Hypertension', 'Diabetes', 'CVD']], annot=True, linewidths=0.3) # 'Asthma', , 'COPD', 'Kidney disease'

"""## Observation(Based on Disease)

*   No adequate pharmacies present for people with hypertension in states montana, idaho, missouri




"""

fig, ax = plt.subplots(figsize=(16, 1))
sns.heatmap(corr_with_pharma.loc[['MALES', 'FEMALES']], annot=True, linewidths=0.3)
fig, ax = plt.subplots(figsize=(16, 2))
sns.heatmap(corr_with_pharma.loc[['VACANT', 'OWNER_OCC', 'RENTER_OCC']], annot=True, linewidths=0.3)

"""## Observation (Gender, Owner/Renter)

*   No descrimination based on Gender

*   No pharmacies near vacant houses like maine, montana, massachusetts, idaho


"""

fig, ax = plt.subplots(figsize=(16, 2))
sns.heatmap(corr_with_pharma.loc[['HOUSEHOLDS',  'HSEHLD_1_M', 'HSEHLD_1_F', 'MARHH_CHD',
       'MARHH_NO_C', 'MHH_CHILD', 'FHH_CHILD', 'FAMILIES']], annot=True, linewidths=0.3)
fig, ax = plt.subplots(figsize=(16, 2))
sns.heatmap(corr_with_pharma.loc[['MED_AGE', 'MED_AGE_M', 'MED_AGE_F', 'AVE_HH_SZ', 'AVE_FAM_SZ']], annot=True, linewidths=0.3)

"""## Observations (Family related columns and aggregated columns)

*   Not Effected by any column





"""

fig, ax = plt.subplots(figsize=(16, 4))
sns.heatmap(corr_with_pharma.loc[['NO_FARMS12','AVE_SIZE12', 'CROP_ACR12', 'AVE_SALE12', 'SQMI', 'Shape__Area',
       'Shape__Length', 'IECC Climate Zone', 'Temp', 'cvd_100k','hypertension_100k', 'STATE_FIPS', 'CNTY_FIPS', 'FIPS', 'POP2010', 'POP10_SQMI',
       'POP2013', 'POP13_SQMI']], annot=True, linewidths=0.3)

"""## Observations(All other columns)

*   Pharmacies are not placed in popultion dense places in states massachusetts, rhode islands and california
*   Rhode islands have correlation with most of the columns compared to other countries

"""





"""#Analysis based on Percentage"""

list1 = ['WHITE', 'BLACK', 'AMERI_ES', 'ASIAN', 'HAWN_PI', 'HISPANIC', 'OTHER', 'MULT_RACE' ,'AGE_UNDER5', 'AGE_5_9', 'AGE_10_14', 'AGE_15_19', 'AGE_20_24', 'AGE_25_34',
         'AGE_35_44', 'AGE_45_54', 'AGE_55_64', 'AGE_65_74', 'AGE_75_84', 'AGE_85_UP' ,'Prevalence of obesity', 'Asthma', 'Hypertension', 'Diabetes', 'CVD', 'COPD',
         'Kidney disease' ,'MALES', 'FEMALES' ,'VACANT', 'OWNER_OCC', 'RENTER_OCC']

for col in list1:
  merge_data[col+"_percent"] = merge_data[col]/ merge_data['POP2010']

corr_with_pharma = merge_data[[col+"_percent" for col in list1]+['count_pharma']].corr().reset_index()[['index', 'count_pharma']]
states_list = list(set(merge_data.State_x))
for s in states_list:
  df_corr = merge_data.loc[merge_data.State_x == s,[col+"_percent" for col in list1]+['count_pharma']].corr(numeric_only=True).reset_index()[['index', 'count_pharma']].rename({'count_pharma' : s}, axis = 1)
  corr_with_pharma = pd.concat([corr_with_pharma, df_corr.drop('index', axis=1)],axis=1)
corr_with_pharma.set_index('index', inplace=True)
corr_with_pharma.index

fig, ax = plt.subplots(figsize=(16, 4))
sns.heatmap(corr_with_pharma.loc[[col+"_percent" for col in ['WHITE', 'BLACK', 'AMERI_ES', 'ASIAN', 'HAWN_PI', 'HISPANIC', 'OTHER', 'MULT_RACE']]], annot=True, linewidths=0.3)
corr_with_pharma.loc[[col+"_percent" for col in ['WHITE', 'BLACK', 'AMERI_ES', 'ASIAN', 'HAWN_PI', 'HISPANIC', 'OTHER', 'MULT_RACE']]].plot(figsize=(15,6))
plt.legend(loc=2, fontsize = 'x-small')

fig, ax = plt.subplots(figsize=(16, 4))
sns.heatmap(corr_with_pharma.loc[[col+"_percent" for col in ['AGE_UNDER5', 'AGE_5_9', 'AGE_10_14', 'AGE_15_19', 'AGE_20_24', 'AGE_25_34', 'AGE_35_44', 'AGE_45_54', 'AGE_55_64', 'AGE_65_74', 'AGE_75_84', 'AGE_85_UP']]], annot=True, linewidths=0.3)
corr_with_pharma.loc[[col+"_percent" for col in ['AGE_UNDER5', 'AGE_5_9', 'AGE_10_14', 'AGE_15_19', 'AGE_20_24', 'AGE_25_34', 'AGE_35_44', 'AGE_45_54', 'AGE_55_64', 'AGE_65_74', 'AGE_75_84', 'AGE_85_UP']]].plot(figsize=(15,6))
plt.legend(loc=2, fontsize = 'x-small')

fig, ax = plt.subplots(figsize=(16, 3))
sns.heatmap(corr_with_pharma.loc[[col+"_percent" for col in ['Prevalence of obesity', 'Hypertension', 'Diabetes', 'CVD']]], annot=True, linewidths=0.3) # 'Asthma', , 'COPD', 'Kidney disease'

fig, ax = plt.subplots(figsize=(16, 1))
sns.heatmap(corr_with_pharma.loc[[col+"_percent" for col in ['MALES', 'FEMALES']]], annot=True, linewidths=0.3)
fig, ax = plt.subplots(figsize=(16, 2))
sns.heatmap(corr_with_pharma.loc[[col+"_percent" for col in ['VACANT', 'OWNER_OCC', 'RENTER_OCC']]], annot=True, linewidths=0.3)



"""# Preprocessing"""

print(merge_data.shape)
print()
print(merge_data.dtypes)
print()
merge_data.describe().T

corr_with_pharma = merge_data.corr().reset_index()[['index', 'count_pharma']]
states_list = list(set(merge_data.State_x))
for s in states_list:
  df_corr = merge_data[merge_data.State_x == s].corr(numeric_only=True).reset_index()[['index', 'count_pharma']].rename({'count_pharma' : s}, axis = 1)
  corr_with_pharma = pd.concat([corr_with_pharma, df_corr.drop('index', axis=1)],axis=1)
corr_with_pharma.set_index('index', inplace=True)
corr_with_pharma.index

print("Total colummns : " + str(corr_with_pharma.shape[0]))
print("After removing categorical columns : " + str(corr_with_pharma['count_pharma'].dropna().shape[0]))
print("After removing columns which have correlation with categorical column between (-0.1, 0.1) : " + str(corr_with_pharma[abs(corr_with_pharma['count_pharma'])>0.1].dropna().shape[0]))

drop_columns = list(corr_with_pharma[abs(corr_with_pharma['count_pharma'])>0.1].dropna().index)
new_preprocess_data = merge_data[drop_columns]
print(new_preprocess_data.shape)

"""## Traintest split, Model"""

import numpy as np
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(new_preprocess_data.drop('count_pharma', axis=1), new_preprocess_data['count_pharma'], test_size=0.2, random_state=42)

def adj_r2():
  return 1 - ((1-r2_score(y_test, y_pred))*(len(y_test)-1)/(len(y_test)-X_test.shape[1]-1))

from sklearn.linear_model import LinearRegression, ElasticNet, BayesianRidge
from sklearn.tree import DecisionTreeRegressor
from sklearn.ensemble import GradientBoostingRegressor
from xgboost.sklearn import XGBRegressor
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
def model_results(X_train, y_train, X_test, y_test):
  models = {'Linear Regression' : LinearRegression(), 'Elastic Regression' : ElasticNet(), 'Bayesian Ridge Regression' : BayesianRidge(),
          'XGB Regressor' : XGBRegressor(), 'GradientBoosting Regressor' : GradientBoostingRegressor(), 'DecisionTree Regressor' : DecisionTreeRegressor(max_depth = 5)}
  df_results = pd.DataFrame(columns = ['R_square', 'Adj_R_square', 'MAE', 'MSE'])
  for key in models.keys():
    model = models[key]
    model.fit(X_train, y_train)
    y_pred = model.predict(X_test)
    try:
      df_preds = pd.concat([df_preds, pd.DataFrame({'pred '+ key : y_pred})], axis=1)
    except:
      df_preds = pd.DataFrame({'pred '+ key : y_pred})
    df_results = pd.concat([df_results, pd.DataFrame({'R_square' : r2_score(y_test, y_pred),
                                                      'Adj_R_square' : 1 - ((1-r2_score(y_test, y_pred))*(len(y_test)-1)/(len(y_test)-X_test.shape[1]-1)),
                                                      'MSE' : mean_squared_error(y_test , y_pred), 'MAE' : mean_absolute_error(y_test , y_pred)},
                                                      index = [key])], axis=0)
  df_preds['ensemble1'] = (df_preds['pred Linear Regression'] + df_preds['pred Elastic Regression'] + df_preds['pred Bayesian Ridge Regression'])/3
  y_pred = df_preds['ensemble1']
  df_results = pd.concat([df_results, pd.DataFrame({'R_square' : r2_score(y_test, y_pred),
                                                      'Adj_R_square' : 1 - ((1-r2_score(y_test, y_pred))*(len(y_test)-1)/(len(y_test)-X_test.shape[1]-1)),
                                                      'MSE' : mean_squared_error(y_test , y_pred), 'MAE' : mean_absolute_error(y_test , y_pred)},
                                                      index = ['ensemble1'])], axis=0)
  return df_results, df_preds

df_results, df_preds = model_results(X_train, y_train, X_test, y_test)
display(df_results)

"""## PCA & Model"""

import numpy as np
from sklearn.decomposition import PCA
pca = PCA(n_components=6)
X_train_pca = pca.fit_transform(X_train)
X_test_pca = pca.transform(X_test)
print(pca.explained_variance_ratio_)
print(sum(pca.explained_variance_ratio_))

df_results, df_preds = model_results(X_train_pca, y_train, X_test_pca, y_test)
display(df_results)

"""#Testing on 'north carolina', 'oregon', 'alaska'"""

other_data = merge_data1[(merge_data1['STATE_NAME'] == 'north carolina') | (merge_data1['STATE_NAME'] == 'oregon') | (merge_data1['STATE_NAME'] == 'alaska')]
list1 = ['WHITE', 'BLACK', 'AMERI_ES', 'ASIAN', 'HAWN_PI', 'HISPANIC', 'OTHER', 'MULT_RACE' ,'AGE_UNDER5', 'AGE_5_9', 'AGE_10_14', 'AGE_15_19', 'AGE_20_24', 'AGE_25_34',
         'AGE_35_44', 'AGE_45_54', 'AGE_55_64', 'AGE_65_74', 'AGE_75_84', 'AGE_85_UP' ,'Prevalence of obesity', 'Asthma', 'Hypertension', 'Diabetes', 'CVD', 'COPD',
         'Kidney disease' ,'MALES', 'FEMALES' ,'VACANT', 'OWNER_OCC', 'RENTER_OCC']

for col in list1:
  other_data[col+"_percent"] = other_data[col]/ other_data['POP2010']

other_data = other_data[drop_columns+['STATE_NAME', 'county']]

for i in ['north carolina', 'oregon', 'alaska']:
  print("Model Accuracies of " + i)
  other_data2 = other_data[other_data['STATE_NAME'] == i]
  X_other = other_data2.drop(['count_pharma', 'STATE_NAME', 'county'], axis = 1)
  y_other = other_data2['count_pharma']
  X_other_pca = pca.transform(X_other)
  df_results, df_preds = model_results(X_train_pca, y_train, X_other_pca, y_other)
  display(df_results)
  print()

import plotly.express as px
for i in ['north carolina', 'oregon', 'alaska']:
  other_data2 = other_data[other_data['STATE_NAME'] == i]
  X_other = other_data2.drop(['count_pharma', 'STATE_NAME','county'], axis = 1)
  y_other = other_data2['count_pharma']
  X_other_pca = pca.transform(X_other)
  model = LinearRegression()
  model.fit(X_train_pca, y_train)
  y_pred = model.predict(X_other_pca)
  df_out = pd.DataFrame({'County' : other_data2['county'], 'Additional pharmacies needed' : list(y_pred - y_other), 'x_points' : [i for i in range(len(y_other))]})
  fig = px.scatter(df_out, x="x_points", y="Additional pharmacies needed", text="County", log_x=True, size_max=100, color="Additional pharmacies needed")
  fig.update_traces(textposition='top center')
  fig.update_layout(title_text='Pharmacies needed in Each county : '+i, title_x=0.5)
  fig.show()

